{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":598009,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":447850,"modelId":464270}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport gradio as gr\n\n\ndef load_model(model_path):\n    model = models.densenet121(pretrained=False)\n    num_features = model.classifier.in_features\n    model.classifier = nn.Linear(num_features, 4)  # 4 classes\n    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n    model.eval()\n    return model\n\nmodel = load_model(\"/kaggle/input/brain_tumor_mri_model/pytorch/default/1/brain_tumor_densenet_best.pth\")\nlabels = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n\n\ndef predict(image):\n    if image is None:\n        return \"Please upload an MRI scan image.\"\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n\n    image = transform(image).unsqueeze(0)\n    with torch.no_grad():\n        outputs = model(image)\n        _, pred = torch.max(outputs, 1)\n    tumor_type = labels[pred.item()]\n    return f\"Predicted Category: {tumor_type}\"\n\n\n# 3. Gradio UI(gpt)\n\ncustom_css = \"\"\"\n.gradio-container {\n    background-color: #f9fafc;\n    font-family: 'Inter', sans-serif;\n    color: #1e293b;\n}\nh1, h2, h3 {\n    color: #0f172a !important;\n}\n.gr-button {\n    background-color: #2563eb !important;\n    color: white !important;\n    border-radius: 8px !important;\n    font-weight: 600 !important;\n}\nfooter {\n    visibility: hidden;\n}\n\"\"\"\n\ndemo = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Brain MRI Scan\"),\n    outputs=gr.Textbox(label=\"AI Diagnostic Result\"),\n    title=\"AI-Powered Brain Tumor Classification System\",\n    description=(\n        \"This AI-driven diagnostic tool leverages a DenseNet121 deep learning model to classify \"\n        \"MRI scans into one of four categories: Glioma, Meningioma, Pituitary, or No Tumor. \"\n        \"Designed for clinical decision support and healthcare research environments.\"\n    ),\n    theme=\"gradio/soft\",\n    css=custom_css,\n    allow_flagging=\"never\",\n    examples=None\n)\n\n\n\n\nif __name__ == \"__main__\":\n    demo.launch(server_name=\"0.0.0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T13:50:56.013259Z","iopub.execute_input":"2025-10-07T13:50:56.013772Z","iopub.status.idle":"2025-10-07T13:50:57.336928Z","shell.execute_reply.started":"2025-10-07T13:50:56.013744Z","shell.execute_reply":"2025-10-07T13:50:57.336361Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://0.0.0.0:7861\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://fcec7065ca9db92a7e.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://fcec7065ca9db92a7e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}